{"paragraphs":[{"text":"import com.datastax.spark.connector._\nimport com.datastax.spark.connector.cql.CassandraConnector\nimport org.apache.spark.sql.cassandra._\nimport scala.util.Try\n","user":"anonymous","dateUpdated":"2018-02-20T16:53:36-0800","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519174405701_-2035344234","id":"20180220-112525_1636082280","dateCreated":"2018-02-20T16:53:25-0800","dateStarted":"2018-02-20T16:53:36-0800","dateFinished":"2018-02-20T16:53:36-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:809"},{"title":"Copy one Cassandra Table to A New Table","text":"// Practice creating new tables from existing Dataframes\n\nval inputData = spark\n  .range(0, 100)\n  .select('id.as(\"k\"), 'id.as(\"c\"), 'id.as(\"v\"))\n  \n//Create a table\nTry(inputData.createCassandraTable(\"test\", \"source\", Some(Seq(\"k\")), Some(Seq(\"c\"))))\n\ninputData\n  .write\n  .cassandraFormat(\"source\",\"test\")\n  .mode(\"append\")\n  .save()\n  \n//Create Destination Table\n//Fill in code here\n//Copy from Source to Destination\n\nprintln (spark.read.cassandraFormat(\"destination\", \"test\").load().count)\nspark.read.cassandraFormat(\"destination\", \"test\").load().count == 100 // Should be true :)","user":"anonymous","dateUpdated":"2018-02-21T07:45:20-0800","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519174405701_-2035344234","id":"20180220-111903_1686365766","dateCreated":"2018-02-20T16:53:25-0800","dateStarted":"2018-02-21T07:45:04-0800","dateFinished":"2018-02-21T07:45:05-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:810"},{"title":"Use SQL to create a Cassandra Dataframe","text":"// Practice the SQL API\nval sql = \"\"\"Your SQL Here\"\"\"\n\nval df = spark.sql(sql).count == 100","dateUpdated":"2018-02-20T16:53:25-0800","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519174405701_-2035344234","id":"20180220-111951_705527584","dateCreated":"2018-02-20T16:53:25-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:811"},{"title":"Change the number of Partitions used in Spark","text":"// Can change the read parameters for a Dataframe reader?\nval df = spark.read.cassandraFormat(\"tab\", \"test\").load\n\nval originalPartitionCount = df.rdd.partitions.size\n\nval df2 = ???\n\nval newPartitionCount = df2.rdd.partitions.size\n\noriginalPartitionCount != newPartitionCount\ndf.count == df2.count","dateUpdated":"2018-02-20T16:53:25-0800","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519174405701_-2035344234","id":"20180220-112827_262208189","dateCreated":"2018-02-20T16:53:25-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:812"},{"title":"Join Data from a CSV with Data in  Cassandra","text":"val inputData = spark\n  .range(1, 10)\n  .select('id.as(\"k\"))\n  \n// Write inputData to a local file file://something.csv\ninputData.write //??\n// Make a dataframe which reads from this file\nval dfFromFile = spark.read //?? \n// Make a dataframe which reads from our test.source Cassandra table\nval dfFromCassandra = spark.read //??\n// Join them together on column k\nval join = //??\n\njoin.count == 10","dateUpdated":"2018-02-20T16:53:25-0800","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519174405701_-2035344234","id":"20180220-113256_1239096648","dateCreated":"2018-02-20T16:53:25-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:813"},{"title":"Fix the Pushdown","text":"// Why isn't the clustering key pushed down?\nval cc = CassandraConnector(spark.sparkContext.getConf)\ncc.withSessionDo{ session => \n  session.execute(\"CREATE TABLE IF NOT EXISTS test.fun (k int, ts timestamp, v int, PRIMARY KEY (k, ts))\")\n}\n\nspark.read.cassandraFormat(\"fun\",\"test\").load().filter('ts < \"2018-03-30\") // This will not push down. How do you find out? How do you fix it?","dateUpdated":"2018-02-20T16:53:25-0800","config":{"tableHide":false,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519174405702_-2034189988","id":"20180220-113848_526833318","dateCreated":"2018-02-20T16:53:25-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:814"},{"dateUpdated":"2018-02-20T16:53:25-0800","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519174405702_-2034189988","id":"20180220-151808_1208184798","dateCreated":"2018-02-20T16:53:25-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:815"}],"name":"Challenges","id":"2D9QF8Z6P","angularObjects":{"2D9MRXK9C:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}