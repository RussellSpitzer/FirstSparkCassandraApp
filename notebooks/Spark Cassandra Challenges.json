{"paragraphs":[{"text":"import com.datastax.spark.connector._\nimport com.datastax.spark.connector.cql.CassandraConnector\nimport org.apache.spark.sql.cassandra._\nimport scala.util.Try\n\n\n//Helpful Links\n//http://spark.apache.org/docs/latest/api/scala/index.html#package Spark Scala API\n//https://github.com/datastax/spark-cassandra-connector/tree/master/doc  Spark Cassandra Connector Docs","user":"anonymous","dateUpdated":"2018-04-25T17:24:39-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519154725645_1951052652","id":"20180220-112525_1636082280","dateCreated":"2018-02-20T11:25:25-0600","dateStarted":"2018-04-25T17:24:39-0500","dateFinished":"2018-04-25T17:25:29-0500","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:10357"},{"title":"Copy one Cassandra Table to A New Table","text":"// Practice changing a C* Partition Key with a DataFrame Copy\n\nval inputData = spark\n  .range(1, 100)\n  .select('id.as(\"k\"), 'id.as(\"c\"), 'id.as(\"v\"))\n  \n//Create a table\nTry(inputData.createCassandraTable(\"test\", \"source\", Some(Seq(\"k\")), Some(Seq(\"c\"))))\n\ninputData\n  .write\n  .cassandraFormat(\"tab\",\"test\")\n  .mode(\"append\")\n  .save()\n\n\n// Your Code Here\n\n//Create a Table (but with a different Partition Key)\n//Copy the data from \"test.source\" to your new table (\"test.destination\")\n\nspark.read.cassandraFormat(\"destination\", \"test\").load().count == 200 // Should be true :)","user":"anonymous","dateUpdated":"2018-04-25T17:33:47-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519154343652_-2143579772","id":"20180220-111903_1686365766","dateCreated":"2018-02-20T11:19:03-0600","dateStarted":"2018-04-25T17:33:21-0500","dateFinished":"2018-04-25T17:33:25-0500","status":"ERROR","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10358"},{"title":"Use SQL to create a Cassandra Dataframe","text":"// Practice the SQL API\nval sql = \"\"\"Your SQL Here\"\"\"\n\nval df = spark.sql(sql).count == 200","user":"anonymous","dateUpdated":"2018-04-25T17:24:51-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519154391163_1238816","id":"20180220-111951_705527584","dateCreated":"2018-02-20T11:19:51-0600","dateStarted":"2018-04-25T17:25:30-0500","dateFinished":"2018-04-25T17:25:45-0500","status":"ERROR","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10359"},{"title":"Change the number of Partitions used in Spark","text":"// Can change the read parameters for a Dataframe reader?\nval df = spark.read.cassandraFormat(\"tab\", \"test\").load\n\nval originalPartitionCount = df.rdd.partitions.size\n\nval df2 = ???\n\nval newPartitionCount = df2.rdd.partitions.size\n\noriginalPartitionCount != newPartitionCount\ndf.count == df2.count","user":"anonymous","dateUpdated":"2018-04-25T17:24:51-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519154907963_-1648432007","id":"20180220-112827_262208189","dateCreated":"2018-02-20T11:28:27-0600","dateStarted":"2018-04-25T17:25:43-0500","dateFinished":"2018-04-25T17:25:49-0500","status":"ERROR","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10360"},{"title":"Join Data from a CSV with Data in  Cassandra","text":"val inputData = spark\n  .range(1, 10)\n  .select('id.as(\"k\"))\n  \n// Write inputData to a local file file://something.csv\ninputData.write //??\n// Make a dataframe which reads from this file\nval dfFromFile = spark.read //?? \n// Make a dataframe which reads from our test.source Cassandra table\nval dfFromCassandra = spark.read //??\n// Join them together on column k\nval join = //??\n\njoin.count == 10","user":"anonymous","dateUpdated":"2018-04-25T17:24:51-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519155176945_756246974","id":"20180220-113256_1239096648","dateCreated":"2018-02-20T11:32:56-0600","dateStarted":"2018-04-25T17:25:45-0500","dateFinished":"2018-04-25T17:25:52-0500","status":"ERROR","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10361"},{"title":"Fix the Pushdown","text":"// Why isn't the clustering key pushed down?\nval cc = CassandraConnector(spark.sparkContext.getConf)\ncc.withSessionDo{ session => \n  session.execute(\"CREATE TABLE IF NOT EXISTS test.fun (k int, ts timestamp, v int, PRIMARY KEY (k, ts))\")\n}\n\nspark.read.cassandraFormat(\"fun\",\"test\").load().filter('ts < \"2018-03-30\") // This will not push down. How do you find out? How do you fix it?","user":"anonymous","dateUpdated":"2018-04-25T17:24:51-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","tableHide":false,"title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519155528290_1599538297","id":"20180220-113848_526833318","dateCreated":"2018-02-20T11:38:48-0600","dateStarted":"2018-04-25T17:25:50-0500","dateFinished":"2018-04-25T17:25:54-0500","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10362"},{"title":"DSE Only Below this Point","text":"//The Best Distribution :)","user":"anonymous","dateUpdated":"2018-04-25T17:24:51-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519168688182_-192544190","id":"20180220-151808_1208184798","dateCreated":"2018-02-20T15:18:08-0600","dateStarted":"2018-04-25T17:25:52-0500","dateFinished":"2018-04-25T17:25:55-0500","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10363"},{"title":"Perform a DSE Direct Join","text":"//Grab k 1 through 10 from test.tab without doing a full table scan!\n//Since our DSE Table is small you may have to apply a hint!\n\nval joinKeys = ???\nassert(joinKeys.count == 10)\n\nval cassandraDF = ???\n\nval joinedDF = ???\njoinedDF.explain\njoinedDF.queryExecution.toString.contains(\"DSE Direct Join\")","user":"anonymous","dateUpdated":"2018-04-25T17:31:44-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1524692795350_-2107393379","id":"20180425-164635_519092328","dateCreated":"2018-04-25T16:46:35-0500","dateStarted":"2018-04-25T17:31:44-0500","dateFinished":"2018-04-25T17:31:44-0500","status":"ERROR","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10364"},{"user":"anonymous","dateUpdated":"2018-04-25T17:24:51-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1524693032475_852817041","id":"20180425-165032_1674880363","dateCreated":"2018-04-25T16:50:32-0500","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10365"}],"name":"Spark Cassandra Challenges","id":"2D9MBAUH5","angularObjects":{"2D9MRXK9C:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}